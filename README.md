slimgest
========

A comprehensive PDF processing and OCR pipeline with multiple execution modes, including world-class document understanding capabilities. The project provides various CLIs for PDF rendering, image preprocessing, and OCR processing using vLLM (DeepSeek OCR) and advanced document analysis models (Nemotron).

Features
--------
1. Single application with intuitive "CLI" feel to be used across the entire enterprise
2. Built in tools and utilities for analyzing and manipulating datasets
3. Built in ability to report issues and share problematic document or PDF output and findings
4. REST API for remote processing capabilities for machines that lack GPU support
5. Kubernetes deployments via Helm charts for enterprise scale
6. Easy and segmented layout and development methods and several utilities provided
7. Crawl, Walk, Run ... start simple with research, grow with local scripts or distributed processing, and thrive with enterprise kubernetes deployments
8. Support tooling for troubleshooting common issues and examining your environment
9. Support for profiling datasets and understanding the common "hotspots" in a deployment/setup
10. Internal benchmarking script for testing achieving max performance under numerous deployment scenarios

Install
-------
We recommend using [uv](https://github.com/astral-sh/uv) for a faster and more reproducible install.

1. **Create a virtual environment (optional but recommended):**
   ```bash
   python -m venv .venv && source .venv/bin/activate
   ```

2. **Install all dependencies + project in editable mode via uv:**
   ```bash
   uv pip install -e .
   ```
   This will install all dependencies specified in the `pyproject.toml`, including those needed for CLI and conversion.

3. **For GPU/cuda and vLLM:**
   The following are not installable directly via PyPI and need to be installed manually. After activating your environment and running `uv pip install -e .`, do:

   - Download the correct vLLM wheel for your architecture from [the vLLM releases page](https://github.com/vllm-project/vllm/releases/tag/v0.8.5).

   - Install necessary CUDA wheels (example for CUDA 11.8 and Python 3.10+):
     ```bash
     uv pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
     uv pip install /path/to/vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
     uv pip install flash-attn==2.7.3 --no-build-isolation
     ```

   *(Adjust the vLLM and CUDA wheel path/names as needed for your Python version and GPU architecture.)*

**Note:** Most dependencies are specified in `pyproject.toml` for easier install with `uv`. However, binary packages like `vllm` and some CUDA-specific wheels cannot be distributed via PyPI and must be installed manually as above.

Available CLI Commands
----------------------

After installation, the following commands are available:

- **`slimgest`** - Main CLI entrypoint
- **`slimgest-deepseek-local`** - DeepSeek OCR local processing
- **`slimgest-local`** - Local OCR pipeline processing
- **`world-class`** - World-class PDF processing with crops and advanced analysis
- **`simple-all-gpu`** - Simplified pipeline with maximum GPU utilization
- **`pdf-utils`** - PDF utility tools (pikepdf-render)
- **`chat-image`** - Interactive image chat interface
- **`pdf2img`** - Convert PDF files to images

Local Processing Pipelines
---------------------------

### Basic Local OCR Pipeline

**Step 1: Convert PDFs to images using pdf2img**

Before running `slimgest-local`, you need to convert PDF files to images, as the local OCR pipeline expects images as input. Use the provided `pdf2img` CLI (installed as part of pip install) to render PDF pages to JPEG images (one image per page):

```bash
pdf2img /path/to/input_pdfs /path/to/input_images_dir
```

- `/path/to/input_pdfs`: Directory containing your PDF files.
- `/path/to/input_images_dir`: Directory where the output images will be stored. Each PDF page will become a JPEG image.

**Step 2: Run the OCR pipeline on images**

Once your images are prepared, run:

```bash
slimgest-local /path/to/input_images_dir /path/to/output_dir --dpi 220
```

- `/path/to/input_images_dir`: The directory containing images generated by `pdf2img` (should contain only `.jpg` images).
- `/path/to/output_dir`: Directory where OCR results will be written.

### World-Class Pipeline

For advanced document processing with crops and page element detection:

```bash
world-class world-class-pdf-with-crops [OPTIONS] INPUT_DIR OUTPUT_DIR
```

This pipeline provides enhanced document understanding capabilities and is ideal for complex document layouts.

### Simple All-GPU Pipeline

For maximum GPU utilization with simplified processing:

```bash
simple-all-gpu simple-all-gpu [OPTIONS] INPUT_DIR OUTPUT_DIR
```

This mode minimizes CPU parallelism while maximizing GPU usage for optimal throughput on GPU-heavy workloads.

### PDF Utilities

Convert and render PDFs using pikepdf:

```bash
pdf-utils pikepdf-render [OPTIONS] INPUT_DIR OUTPUT_DIR
```

Advanced Features
-----------------

- **Nemotron Page Elements**: The project includes support for Nemotron-based page element detection (table structure, page elements analysis)
- **Multi-model Support**: DeepSeek OCR, Nemotron models for document understanding
- **Flexible Rendering**: Support for pypdfium2, PyMuPDF, and pikepdf backends
- **Preprocessing Pipeline**: Image preprocessing with Pillow for optimal OCR results

REST API & Client (TODO/placeholder: Do not use yet ...)
-------------------
-----------
```bash
# Submit and follow a job
slimgest-client submit /path/to/file.pdf --server http://localhost:8000
slimgest-client follow <job_id> --server http://localhost:8000
```

Docker Support
--------------

A Dockerfile is provided for containerized deployment. Build and run:

```bash
docker build -t slimgest .
docker run -it slimgest
```

Project Structure
-----------------

```
slimgest/
├── cli/                    # Command-line interfaces
│   ├── deepseek_local.py  # DeepSeek local processing
│   ├── local.py           # Basic local pipeline
│   ├── client.py          # REST client
│   ├── deepencoder/       # Deep encoder models
│   ├── process/           # Image processing utilities
│   └── utils/             # PDF utilities
├── model/                 # Model implementations
│   └── nemotron_page_elements_v3.py
├── pdf/                   # PDF processing utilities
│   └── pikepdf_render.py
├── simple/                # Simplified GPU pipeline
│   └── simple_all_gpu.py
└── world_class/           # Advanced document processing
    └── world_class_pdf_with_crops.py
```

Notes
-----
- Requires Python 3.10+
- Requires a running vLLM server that serves a DeepSeek OCR-like multimodal chat API at `/v1/chat/completions` (for OCR modes).
- Metrics are collected per phase (render, preprocess, ocr) and shown in the REST client summary and saved in local mode outputs.
- Multiple rendering backends supported: pypdfium2, PyMuPDF, and pikepdf
- Test files included for Nemotron page elements and table structure analysis

CI wheel publishing
------------------
- A GitHub Actions workflow publishes a wheel to **TestPyPI**:
  - **On every commit to `main`**
  - **Nightly** on a schedule
- TestPyPI builds use a PEP 440 dev version like `0.1.0.devYYYYMMDD<run>` so the wheel filename includes the **UTC date** and stays **unique per CI run**.
- The **git short SHA** and **branch/ref** are embedded in the wheel itself (see `slimgest.build_info.get_build_info()`).
- Manual runs can publish to **PyPI** (requires specifying `version`).
- Required GitHub secrets:
  - `TEST_PYPI_API_TOKEN` (used for nightly/TestPyPI publishes)
  - `PYPI_API_TOKEN` (used only when manually publishing to PyPI)

CUDA PyTorch (important)
-----------------------
If you install `torch` from default PyPI, you will often get a **CPU-only** build. Unfortunately, **you cannot reliably force “CUDA torch” purely via `pyproject.toml`** because:
- CUDA-enabled PyTorch wheels are typically served from the PyTorch package index (via `--extra-index-url`), not something a wheel can enforce.
- pip has no mechanism for a wheel to require a specific index URL at install time.

Practical options:
- **Document the install command (recommended)**:
  - Example: `pip install --extra-index-url https://download.pytorch.org/whl/cu121 slimgest`
- **Make GPU deps optional** (extras like `slimgest[cuda]`) and document the same `--extra-index-url` approach.
- **Runtime guardrails**: at startup, detect `torch.cuda.is_available()` and emit a clear error/warning if CUDA is unavailable.
