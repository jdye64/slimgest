slimgest
========

PDF-to-image OCR pipeline with a FastAPI server and CLIs, using a shared core for identical behavior across modes. Rendering via pypdfium, preprocessing via Pillow, OCR via vLLM (DeepSeek OCR).

Install
-------
We recommend using [uv](https://github.com/astral-sh/uv) for a faster and more reproducible install.

1. **Create a virtual environment (optional but recommended):**
   ```bash
   python -m venv .venv && source .venv/bin/activate
   ```

2. **Install all dependencies + project in editable mode via uv:**
   ```bash
   uv pip install -e .
   ```
   This will install all dependencies specified in the `pyproject.toml`, including those needed for CLI and conversion.

3. **For GPU/cuda and vLLM:**
   The following are not installable directly via PyPI and need to be installed manually. After activating your environment and running `uv pip install -e .`, do:

   - Download the correct vLLM wheel for your architecture from [the vLLM releases page](https://github.com/vllm-project/vllm/releases/tag/v0.8.5).

   - Install necessary CUDA wheels (example for CUDA 11.8 and Python 3.10+):
     ```bash
     uv pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
     uv pip install /path/to/vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
     uv pip install flash-attn==2.7.3 --no-build-isolation
     ```

   *(Adjust the vLLM and CUDA wheel path/names as needed for your Python version and GPU architecture.)*

**Note:** Most dependencies are specified in `pyproject.toml` for easier install with `uv`. However, binary packages like `vllm` and some CUDA-specific wheels cannot be distributed via PyPI and must be installed manually as above.


Run the server (TODO/placeholder: Do not use yet ...)
--------------
```bash
slimgest-server --host 0.0.0.0 --port 8000 --vllm-url http://localhost:8001
```

Local processing (This is what you want for now)
----------------

**Step 1: Convert PDFs to images using pdf2img**

Before running `slimgest-local`, you need to convert PDF files to images, as the local OCR pipeline expects images as input. Use the provided `pdf2img` CLI (installed as part of pip install) to render PDF pages to JPEG images (one image per page):

```bash
pdf2img /path/to/input_pdfs /path/to/input_images_dir
```

- `/path/to/input_pdfs`: Directory containing your PDF files.
- `/path/to/input_images_dir`: Directory where the output images will be stored. Each PDF page will become a JPEG image.

**Step 2: Run the OCR pipeline on images**

Once your images are prepared, run:

```bash
slimgest-local /path/to/input_images_dir /path/to/output_dir --dpi 220
```

- `/path/to/input_images_dir`: The directory containing images generated by `pdf2img` (should contain only `.jpg` images).
- `/path/to/output_dir`: Directory where OCR results will be written.

REST client (TODO/placeholder: Do not use yet ...)
-----------
```bash
# Submit and follow a job
slimgest-client submit /path/to/file.pdf --server http://localhost:8000
slimgest-client follow <job_id> --server http://localhost:8000
```

Notes
-----
- Requires Python 3.10+
- Requires a running vLLM server that serves a DeepSeek OCR-like multimodal chat API at `/v1/chat/completions`.
- Metrics are collected per phase (render, preprocess, ocr) and shown in the REST client summary and saved in local mode outputs.
